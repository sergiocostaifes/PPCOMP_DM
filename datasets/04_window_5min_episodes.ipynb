{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJf8WWtCoETsyvGnKFYLBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocostaifes/PPCOMP_DM/blob/main/datasets/04_window_5min_episodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04_window_5min_episodes.ipynb\n",
        "\n",
        "## Objetivo\n",
        "Detectar automaticamente **episódios críticos** na série temporal agregada em janelas de **5 minutos**, utilizando limiar estatístico **μ + 2σ**, consolidando intervalos contínuos e gerando métricas por episódio.\n",
        "\n",
        "## Entradas (do Notebook 03)\n",
        "- `window_5min_base.parquet`\n",
        "- `window_5min_series.parquet` (preferencial: série reindexada e contínua)\n",
        "\n",
        "## Saída (deste Notebook)\n",
        "- `episodes_detected.parquet`\n",
        "\n",
        "## Definições\n",
        "- Série temporal com granularidade oficial de **5 min**\n",
        "- Métrica principal para detecção: `failures_total` (fallback: `fail_rate`)\n",
        "- Limiar: `threshold = μ + 2σ`\n",
        "- Episódio: intervalo contínuo em que `metric >= threshold`\n",
        "\n",
        "## Métricas por episódio\n",
        "- `start_ts`, `end_ts`\n",
        "- `duration_windows` (número de janelas)\n",
        "- `duration_minutes` (duration_windows * 5)\n",
        "- `max_value` (intensidade máxima)\n",
        "- `mean_value` (intensidade média)\n",
        "- `sum_value` (intensidade total)\n",
        "- `threshold`, `mu`, `sigma`\n",
        "- `n_peaks` (número de janelas acima do limiar no episódio)\n",
        "\n",
        "## Observações de reprodutibilidade\n",
        "- Bootstrap via Drive + clone/atualização do repositório\n",
        "- Uso de `src.paths` e `ensure_dirs()`\n",
        "- Seed global fixa\n",
        "- Sem dependência de caminhos relativos frágeis"
      ],
      "metadata": {
        "id": "ls1VLx5IXD4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "UuXKS6gLW9kM",
        "outputId": "f8707df8-3789-401d-ddd0-699a33f15605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../03-aed/dataset_consolidado.parquet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-586/4282630867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# ============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../03-aed/dataset_consolidado.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../03-aed/dataset_consolidado.parquet'"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 04_window_5min_episodes.ipynb\n",
        "# Detecção de episódios críticos (μ + 2σ) na série 5-min\n",
        "# Pipeline PPCOMP_DM (Google Cluster Trace)\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# 0) BOOTSTRAP (Colab + Repo)\n",
        "# -----------------------------\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Mount Google Drive (seguro)\n",
        "if not Path(\"/content/drive/MyDrive\").exists():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    print(\"[Bootstrap] Google Drive já montado.\")\n",
        "\n",
        "# Ajuste aqui se o seu repo estiver em outro caminho do Drive\n",
        "REPO_DIR = Path(\"/content/drive/MyDrive/Mestrado/PPCOMP_DM\")\n",
        "GITHUB_REPO = \"https://github.com/sergiocostaifes/PPCOMP_DM.git\"\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[Bootstrap] Clonando repositório em: {REPO_DIR}\")\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, str(REPO_DIR)], check=True)\n",
        "else:\n",
        "    # Atualiza repo sem quebrar notebook se houver alterações locais\n",
        "    try:\n",
        "        print(\"[Bootstrap] Atualizando repositório (git pull)...\")\n",
        "        subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
        "    except Exception as e:\n",
        "        print(\"[Bootstrap] Aviso: não foi possível atualizar via git pull:\", e)\n",
        "\n",
        "# Trabalhar sempre a partir do repo (evita path relativo quebrar)\n",
        "os.chdir(str(REPO_DIR))\n",
        "print(\"[Bootstrap] CWD =\", os.getcwd())\n",
        "\n",
        "# Garantir import do pacote src/\n",
        "repo_str = str(REPO_DIR)\n",
        "if repo_str not in sys.path:\n",
        "    sys.path.insert(0, repo_str)\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "# Paths padronizados\n",
        "from src.paths import PROCESSED_PATH, FEATURES_PATH, ensure_dirs\n",
        "ensure_dirs()\n",
        "\n",
        "print(\"PROCESSED_PATH =\", PROCESSED_PATH)\n",
        "print(\"FEATURES_PATH  =\", FEATURES_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) INPUTS (Notebook 03)\n",
        "# -----------------------------\n",
        "# Preferimos a série contínua (reindexada) para detecção consistente\n",
        "inp_series = FEATURES_PATH / \"window_5min_series.parquet\"\n",
        "inp_base   = FEATURES_PATH / \"window_5min_base.parquet\"\n",
        "\n",
        "if inp_series.exists():\n",
        "    df = pd.read_parquet(inp_series)\n",
        "    source_used = \"window_5min_series.parquet\"\n",
        "elif inp_base.exists():\n",
        "    df = pd.read_parquet(inp_base)\n",
        "    source_used = \"window_5min_base.parquet\"\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Não encontrei inputs do Notebook 03 em FEATURES_PATH.\\n\"\n",
        "        f\"Esperado: {inp_series} (preferencial) ou {inp_base}.\\n\"\n",
        "        f\"Rode o 03_window_5min_base.ipynb primeiro.\"\n",
        "    )\n",
        "\n",
        "print(\"[Input] Usando:\", source_used)\n",
        "print(\"[Input] shape:\", df.shape)\n",
        "df.head()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) NORMALIZAÇÃO MÍNIMA DE COLUNAS\n",
        "# -----------------------------\n",
        "# Esperado: minute_bucket e métricas agregadas\n",
        "# Métrica principal: failures_total (fallback fail_rate)\n",
        "# Também aceitamos events_total / fail_rate conforme README\n",
        "required_time_col = \"minute_bucket\"\n",
        "if required_time_col not in df.columns:\n",
        "    # fallback: timestamp\n",
        "    if \"timestamp\" in df.columns:\n",
        "        df[\"minute_bucket\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "    else:\n",
        "        raise KeyError(\"Dataset não contém 'minute_bucket' nem 'timestamp' para base temporal.\")\n",
        "\n",
        "df[\"minute_bucket\"] = pd.to_datetime(df[\"minute_bucket\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"minute_bucket\"]).sort_values(\"minute_bucket\").reset_index(drop=True)\n",
        "\n",
        "metric_candidates = [\"failures_total\", \"fail_rate\"]\n",
        "metric = None\n",
        "for c in metric_candidates:\n",
        "    if c in df.columns:\n",
        "        metric = c\n",
        "        break\n",
        "\n",
        "if metric is None:\n",
        "    raise KeyError(\n",
        "        \"Não encontrei nenhuma métrica de falhas esperada.\\n\"\n",
        "        \"Esperado ao menos uma destas colunas: failures_total, fail_rate.\\n\"\n",
        "        f\"Colunas disponíveis: {list(df.columns)[:50]} ...\"\n",
        "    )\n",
        "\n",
        "print(\"[Detecção] Métrica escolhida:\", metric)\n",
        "\n",
        "# Garantir numérico\n",
        "df[metric] = pd.to_numeric(df[metric], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) DETECÇÃO POR LIMIAR (μ + 2σ)\n",
        "# -----------------------------\n",
        "mu = float(df[metric].mean())\n",
        "sigma = float(df[metric].std(ddof=0))  # população (consistência)\n",
        "threshold = mu + 2.0 * sigma\n",
        "\n",
        "df[\"is_critical\"] = (df[metric] >= threshold).astype(int)\n",
        "\n",
        "print(f\"[Stats] mu={mu:.6f} sigma={sigma:.6f} threshold={threshold:.6f}\")\n",
        "print(\"[Stats] critical windows:\", int(df[\"is_critical\"].sum()), \"de\", len(df))\n",
        "\n",
        "# -----------------------------\n",
        "# 4) CONSOLIDAÇÃO DE INTERVALOS CONTÍNUOS (EPISÓDIOS)\n",
        "# -----------------------------\n",
        "# Episódio = sequência contígua de janelas com is_critical=1\n",
        "episodes = []\n",
        "in_episode = False\n",
        "start_idx = None\n",
        "\n",
        "for i, flag in enumerate(df[\"is_critical\"].values):\n",
        "    if flag == 1 and not in_episode:\n",
        "        in_episode = True\n",
        "        start_idx = i\n",
        "    elif flag == 0 and in_episode:\n",
        "        end_idx = i - 1\n",
        "        episodes.append((start_idx, end_idx))\n",
        "        in_episode = False\n",
        "        start_idx = None\n",
        "\n",
        "# Se terminou dentro de episódio\n",
        "if in_episode and start_idx is not None:\n",
        "    episodes.append((start_idx, len(df) - 1))\n",
        "\n",
        "print(\"[Episodes] Detectados:\", len(episodes))\n",
        "\n",
        "# -----------------------------\n",
        "# 5) MÉTRICAS POR EPISÓDIO\n",
        "# -----------------------------\n",
        "WINDOW_MINUTES = 5\n",
        "\n",
        "rows = []\n",
        "for ep_id, (s, e) in enumerate(episodes, start=1):\n",
        "    seg = df.iloc[s:e+1].copy()\n",
        "\n",
        "    start_ts = seg[\"minute_bucket\"].iloc[0]\n",
        "    end_ts   = seg[\"minute_bucket\"].iloc[-1]\n",
        "\n",
        "    duration_windows = int(len(seg))\n",
        "    duration_minutes = int(duration_windows * WINDOW_MINUTES)\n",
        "\n",
        "    max_value  = float(seg[metric].max())\n",
        "    mean_value = float(seg[metric].mean())\n",
        "    sum_value  = float(seg[metric].sum())\n",
        "\n",
        "    n_peaks = int(seg[\"is_critical\"].sum())  # aqui será igual ao tamanho do seg, mas mantém semântica\n",
        "\n",
        "    rows.append({\n",
        "        \"episode_id\": ep_id,\n",
        "        \"start_ts\": start_ts,\n",
        "        \"end_ts\": end_ts,\n",
        "        \"duration_windows\": duration_windows,\n",
        "        \"duration_minutes\": duration_minutes,\n",
        "        \"metric\": metric,\n",
        "        \"max_value\": max_value,\n",
        "        \"mean_value\": mean_value,\n",
        "        \"sum_value\": sum_value,\n",
        "        \"n_peaks\": n_peaks,\n",
        "        \"threshold\": threshold,\n",
        "        \"mu\": mu,\n",
        "        \"sigma\": sigma\n",
        "    })\n",
        "\n",
        "episodes_df = pd.DataFrame(rows)\n",
        "\n",
        "episodes_df.head()\n",
        "\n",
        "# -----------------------------\n",
        "# 6) SANITY CHECKS\n",
        "# -----------------------------\n",
        "if len(episodes_df) > 0:\n",
        "    # Garantir start <= end\n",
        "    assert (episodes_df[\"start_ts\"] <= episodes_df[\"end_ts\"]).all()\n",
        "    # Ordenação\n",
        "    episodes_df = episodes_df.sort_values([\"start_ts\", \"end_ts\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"[Episodes] shape:\", episodes_df.shape)\n",
        "episodes_df.describe(include=\"all\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) PERSISTÊNCIA (Saída do Notebook 04)\n",
        "# -----------------------------\n",
        "outp = FEATURES_PATH / \"episodes_detected.parquet\"\n",
        "episodes_df.to_parquet(outp, index=False)\n",
        "\n",
        "print(\"[Output] Salvo:\", outp)\n",
        "print(\"[Done] Notebook 04 concluído.\")"
      ]
    }
  ]
}