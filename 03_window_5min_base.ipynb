{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoJZlTvSnS6BYfR9uRDr2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocostaifes/PPCOMP_DM/blob/main/03_window_5min_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfHgBlozWnUQ"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 03_window_5min_base.ipynb\n",
        "# Janelas de 5 minutos (minute_bucket) + agregações básicas + série contínua\n",
        "# =========================\n",
        "\n",
        "# ===== Bootstrap padrão PPCOMP_DM (Colab) =====\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "import sys, subprocess, importlib\n",
        "from pathlib import Path\n",
        "from importlib.machinery import PathFinder\n",
        "\n",
        "REPO_DIR = Path(\"/content/drive/MyDrive/Mestrado/PPCOMP_DM\")\n",
        "GITHUB_REPO = \"https://github.com/sergiocostaifes/PPCOMP_DM.git\"\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, str(REPO_DIR)], check=True)\n",
        "\n",
        "repo_str = str(REPO_DIR)\n",
        "if repo_str in sys.path:\n",
        "    sys.path.remove(repo_str)\n",
        "sys.path.insert(0, repo_str)\n",
        "\n",
        "importlib.invalidate_caches()\n",
        "if PathFinder not in sys.meta_path:\n",
        "    sys.meta_path.append(PathFinder)\n",
        "\n",
        "from importlib import reload\n",
        "import src.paths as _paths\n",
        "reload(_paths)\n",
        "\n",
        "from src.paths import RAW_PATH, PROCESSED_PATH, FEATURES_PATH, REPORTS_PATH, ensure_dirs\n",
        "ensure_dirs()\n",
        "\n",
        "def log(msg: str) -> None:\n",
        "    print(f\"[03_window_5min_base] {msg}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Parâmetros\n",
        "# =========================\n",
        "BUCKET_SEC = 5 * 60\n",
        "BUCKET_US = BUCKET_SEC * 1_000_000  # 300_000_000 µs\n",
        "\n",
        "TOP_EVENTS = [\"FAIL\", \"SCHEDULE\", \"FINISH\", \"ENABLE\", \"LOST\", \"EVICT\", \"KILL\"]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Leitura do dataset limpo\n",
        "# =========================\n",
        "CLEAN_PARQUET = PROCESSED_PATH / \"google_trace_clean.parquet\"\n",
        "assert CLEAN_PARQUET.exists(), f\"Arquivo não encontrado: {CLEAN_PARQUET}\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "df = pd.read_parquet(CLEAN_PARQUET)\n",
        "log(f\"Lido: {CLEAN_PARQUET}\")\n",
        "log(f\"Shape: {df.shape} (linhas, colunas)\")\n",
        "\n",
        "# Sanidade mínima\n",
        "required_cols = [\"t_rel_us\", \"machine_id\", \"collection_id\", \"event\", \"failed\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "assert not missing, f\"Colunas obrigatórias ausentes: {missing}\"\n",
        "\n",
        "df[\"t_rel_us\"] = pd.to_numeric(df[\"t_rel_us\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"t_rel_us\"]).copy()\n",
        "df[\"t_rel_us\"] = df[\"t_rel_us\"].astype(\"int64\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) minute_bucket de 5 minutos\n",
        "# =========================\n",
        "df[\"bucket_id\"] = (df[\"t_rel_us\"] // BUCKET_US).astype(\"int64\")\n",
        "df[\"bucket_start_us\"] = (df[\"bucket_id\"] * BUCKET_US).astype(\"int64\")\n",
        "\n",
        "log(f\"bucket_id min..max: {df['bucket_id'].min()}..{df['bucket_id'].max()}\")\n",
        "log(f\"Buckets distintos: {df['bucket_id'].nunique()}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Extrair features simples (resource_request)\n",
        "#    (sem “engenharia pesada” aqui; isso fica no Notebook 05_feature_engineering)\n",
        "# =========================\n",
        "def _to_dict(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    if isinstance(x, dict):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if not s:\n",
        "            return None\n",
        "        # tenta JSON primeiro\n",
        "        try:\n",
        "            return json.loads(s)\n",
        "        except Exception:\n",
        "            # tenta fallback mínimo para strings tipo \"{'cpus':..., 'memory':...}\"\n",
        "            try:\n",
        "                return eval(s, {\"__builtins__\": {}})\n",
        "            except Exception:\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "if \"resource_request\" in df.columns:\n",
        "    rr = df[\"resource_request\"].map(_to_dict)\n",
        "    df[\"req_cpus\"] = rr.map(lambda d: d.get(\"cpus\") if isinstance(d, dict) else np.nan)\n",
        "    df[\"req_mem\"]  = rr.map(lambda d: d.get(\"memory\") if isinstance(d, dict) else np.nan)\n",
        "else:\n",
        "    df[\"req_cpus\"] = np.nan\n",
        "    df[\"req_mem\"]  = np.nan\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Agregações por bucket (window_5min_base)\n",
        "# =========================\n",
        "base = (\n",
        "    df.groupby(\"bucket_id\", as_index=False)\n",
        "      .agg(\n",
        "          bucket_start_us=(\"bucket_start_us\", \"min\"),\n",
        "          n_events=(\"event\", \"size\"),\n",
        "          n_failed=(\"failed\", \"sum\"),\n",
        "          n_machines=(\"machine_id\", \"nunique\"),\n",
        "          n_collections=(\"collection_id\", \"nunique\"),\n",
        "          mean_priority=(\"priority\", \"mean\") if \"priority\" in df.columns else (\"bucket_id\", \"mean\"),\n",
        "          mean_req_cpus=(\"req_cpus\", \"mean\"),\n",
        "          mean_req_mem=(\"req_mem\", \"mean\"),\n",
        "      )\n",
        ")\n",
        "\n",
        "# event counts (top events) – cria colunas event_<X>_count\n",
        "evt_counts = (\n",
        "    df[df[\"event\"].isin(TOP_EVENTS)]\n",
        "      .groupby([\"bucket_id\", \"event\"])\n",
        "      .size()\n",
        "      .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "# garante todas as colunas TOP_EVENTS existirem\n",
        "for ev in TOP_EVENTS:\n",
        "    if ev not in evt_counts.columns:\n",
        "        evt_counts[ev] = 0\n",
        "\n",
        "evt_counts = evt_counts[TOP_EVENTS].reset_index()\n",
        "evt_counts = evt_counts.rename(columns={ev: f\"event_{ev}_count\" for ev in TOP_EVENTS})\n",
        "\n",
        "base = base.merge(evt_counts, on=\"bucket_id\", how=\"left\")\n",
        "\n",
        "# preenchimento seguro\n",
        "count_cols = [\"n_events\", \"n_failed\", \"n_machines\", \"n_collections\"] + [f\"event_{ev}_count\" for ev in TOP_EVENTS]\n",
        "for c in count_cols:\n",
        "    base[c] = base[c].fillna(0).astype(\"int64\")\n",
        "\n",
        "# ordena\n",
        "base = base.sort_values(\"bucket_id\").reset_index(drop=True)\n",
        "\n",
        "log(f\"window_5min_base shape: {base.shape}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Série contínua (reindex) -> window_5min_series\n",
        "# =========================\n",
        "bmin = int(base[\"bucket_id\"].min())\n",
        "bmax = int(base[\"bucket_id\"].max())\n",
        "\n",
        "full = pd.DataFrame({\"bucket_id\": np.arange(bmin, bmax + 1, dtype=np.int64)})\n",
        "full[\"bucket_start_us\"] = (full[\"bucket_id\"] * BUCKET_US).astype(np.int64)\n",
        "\n",
        "series = full.merge(base, on=[\"bucket_id\", \"bucket_start_us\"], how=\"left\")\n",
        "\n",
        "# colunas de contagem: zeros quando bucket não existia no dataset\n",
        "for c in count_cols:\n",
        "    series[c] = series[c].fillna(0).astype(\"int64\")\n",
        "\n",
        "# métricas contínuas: manter NaN (depois, no FE, você decide interpolar, ffill etc.)\n",
        "for c in [\"mean_priority\", \"mean_req_cpus\", \"mean_req_mem\"]:\n",
        "    if c in series.columns:\n",
        "        series[c] = series[c].astype(\"float64\")\n",
        "\n",
        "log(f\"window_5min_series shape: {series.shape} (contínua)\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) Persistência\n",
        "# =========================\n",
        "BASE_FILE = FEATURES_PATH / \"window_5min_base.parquet\"\n",
        "SERIES_FILE = FEATURES_PATH / \"window_5min_series.parquet\"\n",
        "\n",
        "base.to_parquet(BASE_FILE, index=False, compression=\"snappy\")\n",
        "series.to_parquet(SERIES_FILE, index=False, compression=\"snappy\")\n",
        "\n",
        "log(f\"Salvo: {BASE_FILE}\")\n",
        "log(f\"Salvo: {SERIES_FILE}\")\n",
        "\n",
        "summary03 = {\n",
        "    \"input_file\": str(CLEAN_PARQUET),\n",
        "    \"output_base\": str(BASE_FILE),\n",
        "    \"output_series\": str(SERIES_FILE),\n",
        "    \"bucket_sec\": int(BUCKET_SEC),\n",
        "    \"bucket_us\": int(BUCKET_US),\n",
        "    \"rows_in\": int(len(df)),\n",
        "    \"base_rows\": int(len(base)),\n",
        "    \"series_rows\": int(len(series)),\n",
        "    \"bucket_id_min\": int(bmin),\n",
        "    \"bucket_id_max\": int(bmax),\n",
        "    \"top_events\": TOP_EVENTS,\n",
        "    \"columns_base\": list(base.columns),\n",
        "}\n",
        "\n",
        "summary_file = REPORTS_PATH / \"03_window_5min_base_summary.json\"\n",
        "summary_file.write_text(json.dumps(summary03, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "log(f\"Resumo salvo: {summary_file}\")\n",
        "\n",
        "base.head()"
      ]
    }
  ]
}