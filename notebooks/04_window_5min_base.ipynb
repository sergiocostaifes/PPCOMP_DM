{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwFyaUcbZZ7RputtIwUYfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocostaifes/PPCOMP_DM/blob/main/notebooks/04_window_5min_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Título: 04_window_5min_base.ipynb\n",
        "Objetivo: criar janelas de 5 minutos (minute_bucket) e gerar dataset base agregado para análises e feature engineering.\n",
        "Entrada: 02-processed/google_trace_clean.parquet\n",
        "Saídas:\n",
        "\n",
        "03-features/window_5min_base.parquet\n",
        "\n",
        "03-features/window_5min_series.parquet"
      ],
      "metadata": {
        "id": "ls1VLx5IXD4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuXKS6gLW9kM"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 04_window_5min_base.ipynb\n",
        "# Janelamento 5 minutos + agregações base\n",
        "# =========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CLEAN_PARQUET = PROCESSED_PATH / \"google_trace_clean.parquet\"\n",
        "assert CLEAN_PARQUET.exists(), f\"Não achei: {CLEAN_PARQUET}\"\n",
        "\n",
        "df = pd.read_parquet(CLEAN_PARQUET)\n",
        "log(f\"Carregado: {df.shape}\")\n",
        "\n",
        "# Checagens mínimas\n",
        "required_cols = {\"time\", \"t_rel_us\", \"hour\", \"failed\"}\n",
        "missing = required_cols - set(df.columns)\n",
        "assert not missing, f\"Colunas faltando no dataset limpo: {missing}\"\n",
        "\n",
        "df[[\"time\",\"t_rel_us\",\"hour\",\"failed\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir minute_bucket (5 min)\n",
        "\n",
        "Definição:\n",
        "\n",
        "5 min = 300s = 300.000.000 µs"
      ],
      "metadata": {
        "id": "XVzCsSppXNBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minutos em microssegundos\n",
        "WINDOW_US = 300_000_000  # 300s\n",
        "\n",
        "df[\"minute_bucket\"] = (df[\"t_rel_us\"] // WINDOW_US).astype(\"int64\")\n",
        "\n",
        "log(f\"minute_bucket min..max: {df['minute_bucket'].min()}..{df['minute_bucket'].max()}\")\n",
        "log(f\"Número de buckets distintos: {df['minute_bucket'].nunique()}\")"
      ],
      "metadata": {
        "id": "qLIbH8grXYHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregações base por janela\n",
        "\n",
        "Aqui vamos fazer agregações “universais”, que não dependem de colunas opcionais.\n",
        "\n",
        "events_total: nº de eventos no bucket\n",
        "\n",
        "failures_total: nº de falhas (failed==1) no bucket\n",
        "\n",
        "fail_rate: failures_total / events_total\n",
        "\n",
        "Se existirem colunas como event, machine_id, job_id etc., adicionamos agregações extras sem quebrar."
      ],
      "metadata": {
        "id": "CkREqKM-XaQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group = df.groupby(\"minute_bucket\")\n",
        "\n",
        "window_base = pd.DataFrame({\n",
        "    \"events_total\": group.size(),\n",
        "    \"failures_total\": group[\"failed\"].sum(),\n",
        "})\n",
        "\n",
        "window_base[\"fail_rate\"] = window_base[\"failures_total\"] / window_base[\"events_total\"]\n",
        "\n",
        "# agregações opcionais (se existirem no dataset)\n",
        "if \"event\" in df.columns:\n",
        "    # contagem de eventos FAIL (redundante com failed, mas útil para auditoria)\n",
        "    window_base[\"fail_event_count\"] = group[\"event\"].apply(lambda s: (s == \"FAIL\").sum())\n",
        "\n",
        "if \"machine_id\" in df.columns:\n",
        "    window_base[\"unique_machines\"] = group[\"machine_id\"].nunique()\n",
        "\n",
        "if \"job_id\" in df.columns:\n",
        "    window_base[\"unique_jobs\"] = group[\"job_id\"].nunique()\n",
        "\n",
        "# reset index para virar tabela\n",
        "window_base = window_base.reset_index()\n",
        "\n",
        "log(f\"window_base: {window_base.shape}\")\n",
        "window_base.head()"
      ],
      "metadata": {
        "id": "L8AMd2RxXg7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar série completa contínua (reindex)\n",
        "\n",
        "Isso é importante para:\n",
        "\n",
        "análise temporal\n",
        "\n",
        "detectar episódios depois (sem buracos)\n",
        "\n",
        "padronizar dataset para treino"
      ],
      "metadata": {
        "id": "ghIP-NUNXixq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mb_min = int(window_base[\"minute_bucket\"].min())\n",
        "mb_max = int(window_base[\"minute_bucket\"].max())\n",
        "\n",
        "full_index = pd.DataFrame({\"minute_bucket\": np.arange(mb_min, mb_max + 1, dtype=\"int64\")})\n",
        "\n",
        "window_series = full_index.merge(window_base, on=\"minute_bucket\", how=\"left\").fillna(0)\n",
        "\n",
        "# garantir tipos numéricos corretos\n",
        "for col in window_series.columns:\n",
        "    if col != \"minute_bucket\":\n",
        "        window_series[col] = pd.to_numeric(window_series[col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "log(f\"window_series (completo): {window_series.shape}\")\n",
        "window_series.head()"
      ],
      "metadata": {
        "id": "LWA1tP20Xlv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvar Parquet"
      ],
      "metadata": {
        "id": "nowZa880XnWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_BASE   = FEATURES_PATH / \"window_5min_base.parquet\"\n",
        "OUT_SERIES = FEATURES_PATH / \"window_5min_series.parquet\"\n",
        "\n",
        "window_base.to_parquet(OUT_BASE, index=False, compression=\"snappy\")\n",
        "window_series.to_parquet(OUT_SERIES, index=False, compression=\"snappy\")\n",
        "\n",
        "log(f\"Salvo: {OUT_BASE}\")\n",
        "log(f\"Salvo: {OUT_SERIES}\")"
      ],
      "metadata": {
        "id": "9AG1ZaRQXpuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanidade final rápida"
      ],
      "metadata": {
        "id": "oQn_tMQcXwQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log(\"Sanidade rápida\")\n",
        "log(f\"Total events_total (base): {int(window_base['events_total'].sum())}\")\n",
        "log(f\"Total failures_total (base): {int(window_base['failures_total'].sum())}\")\n",
        "\n",
        "top = window_series.sort_values(\"failures_total\", ascending=False).head(10)\n",
        "top"
      ],
      "metadata": {
        "id": "3Y22gOsaXrp7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}