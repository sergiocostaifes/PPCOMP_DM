{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfTQ7kTOG2y0rRXD9hOati",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocostaifes/PPCOMP_DM/blob/main/notebooks/01_ingest_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrap padrão"
      ],
      "metadata": {
        "id": "mAmkVh7NXuWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Bootstrap padrão PPCOMP_DM (Colab) ---\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "import sys, subprocess, importlib\n",
        "from pathlib import Path\n",
        "from importlib.machinery import PathFinder\n",
        "\n",
        "REPO_DIR = Path(\"/content/drive/MyDrive/Mestrado/PPCOMP_DM\")\n",
        "GITHUB_REPO = \"https://github.com/sergiocostaifes/PPCOMP_DM.git\"\n",
        "\n",
        "# 1) Garantir repo (clone se faltar)\n",
        "if not REPO_DIR.exists():\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, str(REPO_DIR)], check=True)\n",
        "\n",
        "# 2) sys.path com prioridade\n",
        "repo_str = str(REPO_DIR)\n",
        "if repo_str in sys.path:\n",
        "    sys.path.remove(repo_str)\n",
        "sys.path.insert(0, repo_str)\n",
        "\n",
        "# 3) Ajuste do Colab: garantir import normal de pacotes locais\n",
        "importlib.invalidate_caches()\n",
        "if PathFinder not in sys.meta_path:\n",
        "    sys.meta_path.append(PathFinder)\n",
        "\n",
        "# 4) Garantir src como pacote\n",
        "(REPO_DIR / \"src\").mkdir(parents=True, exist_ok=True)\n",
        "init_file = REPO_DIR / \"src\" / \"__init__.py\"\n",
        "if not init_file.exists():\n",
        "    init_file.write_text(\"# src package\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(\"REPO_DIR:\", REPO_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KEAqgt6LehK1",
        "outputId": "0a684f8c-424d-404d-cb6d-8274561d5e69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "REPO_DIR: /content/drive/MyDrive/Mestrado/PPCOMP_DM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código ingest e validate"
      ],
      "metadata": {
        "id": "0DkpRFZej7-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XGAq4-A1S7bq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7dd26211-9e9e-44b3-f120-314340510010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paths carregados:\n",
            "RAW_PATH: /content/drive/MyDrive/Mestrado/02-datasets/01-raw\n",
            "PROCESSED_PATH: /content/drive/MyDrive/Mestrado/02-datasets/02-processed\n",
            "[01_ingest_validate] Arquivo encontrado: /content/drive/MyDrive/Mestrado/02-datasets/01-raw/borg_traces_data.csv\n",
            "[01_ingest_validate] Leitura OK: (405894, 34) (linhas, colunas)\n",
            "[01_ingest_validate] Colunas: ['Unnamed: 0', 'time', 'instance_events_type', 'collection_id', 'scheduling_class', 'collection_type', 'priority', 'alloc_collection_id', 'instance_index', 'machine_id', 'resource_request', 'constraint', 'collections_events_type', 'user', 'collection_name', 'collection_logical_name', 'start_after_collection_ids', 'vertical_scaling', 'scheduler', 'start_time', 'end_time', 'average_usage', 'maximum_usage', 'random_sample_usage', 'assigned_memory', 'page_cache_memory', 'cycles_per_instruction', 'memory_accesses_per_instruction', 'sample_rate', 'cpu_usage_distribution', 'tail_cpu_usage_distribution', 'cluster', 'event', 'failed']\n",
            "[01_ingest_validate] time NaNs antes: 0 | depois de sentinela: 3\n",
            "[01_ingest_validate] time min: 0.0\n",
            "[01_ingest_validate] time median: 1084786677422.0\n",
            "[01_ingest_validate] time max: 2678923967375.0\n",
            "[01_ingest_validate] Observação: time==0 será tratado no 02_clean_normalize (remoção/ajuste conforme regra do projeto).\n",
            "[01_ingest_validate] Concordância event=='FAIL' vs failed==1: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'raw_rows': 405894,\n",
              " 'raw_cols': 33,\n",
              " 'raw_time_nan': 3,\n",
              " 'raw_failed_1_count': 92678,\n",
              " 'raw_event_top': {'FINISH': 92867,\n",
              "  'FAIL': 92678,\n",
              "  'ENABLE': 75907,\n",
              "  'SCHEDULE': 69104,\n",
              "  'LOST': 59515,\n",
              "  'EVICT': 14756,\n",
              "  'KILL': 951,\n",
              "  'UPDATE_PENDING': 111,\n",
              "  'QUEUE': 4,\n",
              "  'UPDATE_RUNNING': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# =========================\n",
        "# 01_ingest_validate.ipynb\n",
        "# Ingestão + Sanidade\n",
        "# =========================\n",
        "\n",
        "# (crítico) garante que o Python não use módulo antigo em cache\n",
        "importlib.invalidate_caches()\n",
        "import os\n",
        "\n",
        "# ===== Imports do projeto =====\n",
        "from importlib import reload\n",
        "import src.paths as _paths\n",
        "reload(_paths)  # garante pegar a versão atual do arquivo\n",
        "\n",
        "from src.paths import RAW_PATH, PROCESSED_PATH, FEATURES_PATH, MODELS_PATH, REPORTS_PATH, ensure_dirs\n",
        "ensure_dirs()\n",
        "\n",
        "print(\"Paths carregados:\")\n",
        "print(\"RAW_PATH:\", RAW_PATH)\n",
        "print(\"PROCESSED_PATH:\", PROCESSED_PATH)\n",
        "\n",
        "def log(msg: str) -> None:\n",
        "    print(f\"[01_ingest_validate] {msg}\")\n",
        "\n",
        "# ===== Libs =====\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 0) Arquivo de entrada\n",
        "CSV_FILE = RAW_PATH / \"borg_traces_data.csv\"\n",
        "\n",
        "if not CSV_FILE.exists():\n",
        "    # diagnóstico detalhado (não fica “cego”)\n",
        "    log(f\"Arquivo não encontrado: {CSV_FILE}\")\n",
        "    log(f\"Listando RAW_PATH: {RAW_PATH}\")\n",
        "    try:\n",
        "        log(str(os.listdir(RAW_PATH)))\n",
        "    except Exception as e:\n",
        "        log(f\"Falha ao listar RAW_PATH: {e}\")\n",
        "    raise FileNotFoundError(f\"Arquivo não encontrado: {CSV_FILE}\")\n",
        "\n",
        "log(f\"Arquivo encontrado: {CSV_FILE}\")\n",
        "\n",
        "# 1) Leitura (opções melhores para Colab)\n",
        "df = pd.read_csv(\n",
        "    CSV_FILE,\n",
        "    low_memory=False\n",
        ")\n",
        "log(f\"Leitura OK: {df.shape} (linhas, colunas)\")\n",
        "log(f\"Colunas: {list(df.columns)}\")\n",
        "\n",
        "# 2) Remover coluna lixo se existir\n",
        "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
        "\n",
        "# 3) time: numérico + limpeza de sentinela (regra Dia 1)\n",
        "if \"time\" not in df.columns:\n",
        "    raise KeyError(\"Coluna 'time' não encontrada no dataset.\")\n",
        "\n",
        "df[\"time\"] = pd.to_numeric(df[\"time\"], errors=\"coerce\")\n",
        "sentinel_cutoff = 1e16\n",
        "n_before = int(df[\"time\"].isna().sum())\n",
        "df.loc[df[\"time\"] > sentinel_cutoff, \"time\"] = np.nan\n",
        "n_after = int(df[\"time\"].isna().sum())\n",
        "\n",
        "log(f\"time NaNs antes: {n_before} | depois de sentinela: {n_after}\")\n",
        "log(f\"time min: {df['time'].min()}\")\n",
        "log(f\"time median: {df['time'].median()}\")\n",
        "log(f\"time max: {df['time'].max()}\")\n",
        "if df[\"time\"].min() == 0:\n",
        "    log(\"Observação: time==0 será tratado no 02_clean_normalize (remoção/ajuste conforme regra do projeto).\")\n",
        "\n",
        "# 5) Validar falha: FAIL vs failed\n",
        "if \"event\" in df.columns and \"failed\" in df.columns:\n",
        "    concord = (df[\"event\"].eq(\"FAIL\") == df[\"failed\"].eq(1)).mean()\n",
        "    log(f\"Concordância event=='FAIL' vs failed==1: {concord:.4f}\")\n",
        "else:\n",
        "    log(\"Colunas event/failed não encontradas; pulando checagem de concordância.\")\n",
        "\n",
        "# 6) Relatório curto (visão RAW - antes da seleção KEEP_COLS)\n",
        "summary = {\n",
        "    \"raw_rows\": int(df.shape[0]),\n",
        "    \"raw_cols\": int(df.shape[1]),\n",
        "    \"raw_time_nan\": int(df[\"time\"].isna().sum()),\n",
        "    \"raw_failed_1_count\": int((df[\"failed\"] == 1).sum()) if \"failed\" in df.columns else None,\n",
        "    \"raw_event_top\": df[\"event\"].value_counts().head(10).to_dict() if \"event\" in df.columns else None,\n",
        "}\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistência do dataset validado"
      ],
      "metadata": {
        "id": "MY8h5sf-6RYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Persistência do dataset validado (Parquet)\n",
        "# =========================\n",
        "\n",
        "# Confirmar que o pyarrow está instalado\n",
        "import importlib.util, subprocess, sys\n",
        "if importlib.util.find_spec(\"pyarrow\") is None:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"pyarrow\"], check=True)\n",
        "\n",
        "# KEEP_COLS = [ # SIMPLE\n",
        "#     \"time\",\n",
        "#     \"failed\",\n",
        "#     \"event\",\n",
        "#     \"machine_id\",\n",
        "#     \"collection_id\",\n",
        "#     \"instance_index\",\n",
        "#     \"priority\",\n",
        "#     \"scheduling_class\",\n",
        "#     \"start_time\",\n",
        "#     \"end_time\",\n",
        "# ]\n",
        "\n",
        "KEEP_COLS = [  # EXTENDED\n",
        "    \"time\",\"failed\",\"event\",\"machine_id\",\"collection_id\",\"instance_index\",\n",
        "    \"priority\",\"scheduling_class\",\"start_time\",\"end_time\",\n",
        "    \"resource_request\",\"average_usage\",\"maximum_usage\",\"random_sample_usage\",\n",
        "    \"assigned_memory\",\"page_cache_memory\",\n",
        "    \"cycles_per_instruction\",\"memory_accesses_per_instruction\",\n",
        "    \"sample_rate\",\"cluster\",\"scheduler\"\n",
        "]\n",
        "\n",
        "# Tipos recomendados (aplicados depois da leitura)\n",
        "CAT_COLS = [c for c in [\"event\",\"cluster\",\"scheduler\",\"scheduling_class\",\"priority\"] if c in KEEP_COLS]\n",
        "FLOAT32_COLS = [c for c in [\n",
        "    \"average_usage\",\"maximum_usage\",\"random_sample_usage\",\n",
        "    \"assigned_memory\",\"page_cache_memory\",\n",
        "    \"cycles_per_instruction\",\"memory_accesses_per_instruction\",\n",
        "    \"sample_rate\"\n",
        "] if c in KEEP_COLS]\n",
        "\n",
        "INT_COLS = [c for c in [\"machine_id\",\"collection_id\",\"instance_index\",\"start_time\",\"end_time\"] if c in KEEP_COLS]\n",
        "\n",
        "# Releitura do CSV só com as colunas necessárias (mais rápido e leve)\n",
        "df = pd.read_csv(CSV_FILE, usecols=KEEP_COLS, low_memory=False)\n",
        "\n",
        "# Limpeza time + sentinela\n",
        "df[\"time\"] = pd.to_numeric(df[\"time\"], errors=\"coerce\")\n",
        "sentinel_cutoff = 1e16\n",
        "df.loc[df[\"time\"] > sentinel_cutoff, \"time\"] = np.nan\n",
        "\n",
        "# failed\n",
        "df[\"failed\"] = pd.to_numeric(df[\"failed\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
        "\n",
        "# Categóricas\n",
        "for c in CAT_COLS:\n",
        "    df[c] = df[c].astype(\"category\")\n",
        "\n",
        "# Inteiros (nullable, para não estourar se houver NaN)\n",
        "for c in INT_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# Float32 (reduz memória)\n",
        "for c in FLOAT32_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
        "\n",
        "# Remover linhas sem time (recomendado para o pipeline)\n",
        "n_before = len(df)\n",
        "df = df.dropna(subset=[\"time\"]).copy()\n",
        "df[\"time\"] = df[\"time\"].astype(\"int64\")\n",
        "print(f\"Drop time NaN: {n_before} -> {len(df)}\")\n",
        "\n",
        "# =========================\n",
        "# Salvar Parquet validado\n",
        "# =========================\n",
        "VALIDATED_FILE = PROCESSED_PATH / \"trace_raw_validated.parquet\"\n",
        "df.to_parquet(VALIDATED_FILE, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
        "\n",
        "log(f\"Salvo: {VALIDATED_FILE}\")\n",
        "log(f\"Tamanho (MB): {round(VALIDATED_FILE.stat().st_size/1024/1024, 2)}\")\n",
        "\n",
        "# =========================\n",
        "# Atualizar summary final\n",
        "# =========================\n",
        "summary[\"parquet_rows\"] = int(len(df))\n",
        "summary[\"parquet_cols\"] = int(df.shape[1])\n",
        "summary[\"kept_cols\"] = list(df.columns)\n",
        "summary[\"validated_file\"] = str(VALIDATED_FILE)\n",
        "\n",
        "# =========================\n",
        "# Persistir summary\n",
        "# =========================\n",
        "import json\n",
        "summary_file = REPORTS_PATH / \"01_ingest_validate_summary.json\"\n",
        "summary_file.write_text(\n",
        "    json.dumps(summary, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "log(f\"Resumo salvo: {summary_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eB9dPw_xwgvz",
        "outputId": "7323818e-8c31-40b2-aa7f-523e478a68fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drop time NaN: 405894 -> 405891\n",
            "[01_ingest_validate] Salvo: /content/drive/MyDrive/Mestrado/02-datasets/02-processed/trace_raw_validated.parquet\n",
            "[01_ingest_validate] Tamanho (MB): 12.3\n",
            "[01_ingest_validate] Resumo salvo: /content/drive/MyDrive/Mestrado/04-reports/01_ingest_validate_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Achados do Notebook 01 — Ingestão e Validação Inicial\n",
        "\n",
        "## Leitura e estrutura do dataset bruto\n",
        "- Arquivo de entrada: `borg_traces_data.csv`\n",
        "- Total de registros lidos: **405.894**\n",
        "- Total de colunas após remoção de coluna lixo (`Unnamed: 0`): **33**\n",
        "\n",
        "Após remoção de `time` inválido (NaN decorrente de sentinela):\n",
        "- Registros persistidos em parquet validado: **405.891**\n",
        "\n",
        "## Sanidade temporal\n",
        "- `time` convertido para numérico (int64)\n",
        "- Valores acima do sentinela (`1e16`) tratados como inválidos\n",
        "- `time == 0` identificado como caso especial (tratado formalmente no Notebook 02)\n",
        "\n",
        "## Consistência semântica de falhas\n",
        "- Concordância entre `event == \"FAIL\"` e `failed == 1`: **100%**\n",
        "- Isso valida a consistência lógica entre a variável categórica de evento e a variável binária de falha.\n",
        "\n",
        "## Distribuição dos principais eventos\n",
        "Top eventos observados:\n",
        "- FINISH\n",
        "- FAIL\n",
        "- ENABLE\n",
        "- SCHEDULE\n",
        "- LOST\n",
        "- EVICT\n",
        "- KILL\n",
        "\n",
        "A presença equilibrada entre eventos de término e falha sugere comportamento dinâmico realista do sistema.\n",
        "\n",
        "## Persistência e rastreabilidade\n",
        "- Dataset validado salvo como: `trace_raw_validated.parquet`\n",
        "- Summary salvo em: `01_ingest_validate_summary.json`\n",
        "- Tipos ajustados para otimização de memória (category, float32, Int64)\n",
        "\n",
        "## Conclusão metodológica\n",
        "Este notebook estabelece:\n",
        "- Integridade estrutural do dataset\n",
        "- Consistência semântica das variáveis críticas\n",
        "- Base confiável para normalização temporal (Notebook 02)"
      ],
      "metadata": {
        "id": "bZgT2Ct6DzT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}