{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8YgZfB++pHb3PtPZWJSsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocostaifes/PPCOMP_DM/blob/main/notebooks/02_clean_normalize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83wMrmavWLo2"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 02_clean_normalize.ipynb\n",
        "# Limpeza + Normalização + Parquet limpo\n",
        "# =========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Preferir parquet validado (mais rápido). Se não existir, cai no CSV.\n",
        "VALIDATED_PARQUET = PROCESSED_PATH / \"trace_raw_validated.parquet\"\n",
        "CSV_FILE = RAW_PATH / \"borg_traces_data.csv\"\n",
        "\n",
        "if VALIDATED_PARQUET.exists():\n",
        "    df = pd.read_parquet(VALIDATED_PARQUET)\n",
        "    log(\"Lido do parquet validado.\")\n",
        "else:\n",
        "    df = pd.read_csv(CSV_FILE)\n",
        "    df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
        "    df[\"time\"] = pd.to_numeric(df[\"time\"], errors=\"coerce\")\n",
        "    df.loc[df[\"time\"] > 1e16, \"time\"] = np.nan\n",
        "    log(\"Lido do CSV (fallback).\")\n",
        "\n",
        "# 2) Remover time NaN\n",
        "df = df.dropna(subset=[\"time\"]).copy()\n",
        "\n",
        "# 3) Criar tempo relativo e hour (como nos seus dias 2 e 3)\n",
        "t0 = df[\"time\"].min()\n",
        "df[\"t_rel_us\"] = df[\"time\"] - t0\n",
        "\n",
        "# teto conservador: 3e12 us (~34 dias) usado por você no Dia 3 :contentReference[oaicite:7]{index=7}\n",
        "df = df[(df[\"t_rel_us\"] >= 0) & (df[\"t_rel_us\"] <= 3.0e12)].copy()\n",
        "\n",
        "df[\"hour\"] = (df[\"t_rel_us\"] // 3_600_000_000).astype(\"int64\")\n",
        "\n",
        "log(f\"Horas (min..max): {df['hour'].min()}..{df['hour'].max()}\")\n",
        "log(f\"Registros após limpeza: {len(df)}\")\n",
        "\n",
        "# 4) Remover hora 0 (artefato identificado no Dia 2) :contentReference[oaicite:8]{index=8}\n",
        "before = len(df)\n",
        "df = df[df[\"hour\"] != 0].copy()\n",
        "after = len(df)\n",
        "log(f\"Removida hour=0: {before-after} registros removidos.\")\n",
        "\n",
        "# 5) (Opcional) Dedupe se necessário — aqui só se fizer sentido pra você\n",
        "# df = df.drop_duplicates()\n",
        "\n",
        "# 6) Salvar dataset limpo definitivo\n",
        "CLEAN_PARQUET = PROCESSED_PATH / \"google_trace_clean.parquet\"\n",
        "df.to_parquet(CLEAN_PARQUET, index=False, compression=\"snappy\")\n",
        "log(f\"Salvo: {CLEAN_PARQUET}\")\n",
        "\n",
        "df.head()"
      ]
    }
  ]
}